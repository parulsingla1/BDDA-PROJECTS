{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('CoronavirusNLP').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('Corona_NLP_train.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "|UserName|  ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "|    3799|       48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|    3800|       48752|                  UK|16-03-2020|advice Talk to yo...| Positive|\n",
      "|    3801|       48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|\n",
      "|    3802|       48754|                null|16-03-2020|My food stock is ...|     null|\n",
      "|  PLEASE| don't panic| THERE WILL BE EN...|      null|                null|     null|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68046, 6)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('Tweet_length', length(df['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+----------+--------------------+---------+------------+\n",
      "|UserName|  ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|Tweet_length|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+------------+\n",
      "|    3799|       48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|         111|\n",
      "|    3800|       48752|                  UK|16-03-2020|advice Talk to yo...| Positive|         237|\n",
      "|    3801|       48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|         131|\n",
      "|    3802|       48754|                null|16-03-2020|My food stock is ...|     null|          51|\n",
      "|  PLEASE| don't panic| THERE WILL BE EN...|      null|                null|     null|        null|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments=['Positive','Negative','Neutral','Extremely Positive','Extremely Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.filter(df.Sentiment.isin(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         Sentiment|\n",
      "+------------------+\n",
      "|Extremely Negative|\n",
      "|           Neutral|\n",
      "|          Positive|\n",
      "|          Negative|\n",
      "|Extremely Positive|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('Sentiment').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|         Sentiment|count|\n",
      "+------------------+-----+\n",
      "|Extremely Negative| 3751|\n",
      "|           Neutral| 5224|\n",
      "|          Positive| 7718|\n",
      "|          Negative| 6857|\n",
      "|Extremely Positive| 4412|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|Tweet_length|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|         111|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...| Positive|         237|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|         131|\n",
      "|    3804|     48756|ÃœT: 36.319708,-82...|16-03-2020|As news of the re...| Positive|         249|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...| Positive|         184|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27962, 7)\n"
     ]
    }
   ],
   "source": [
    "print((data.count(),len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+-------------+---------+------------+\n",
      "|UserName|ScreenName|Location|TweetAt|OriginalTweet|Sentiment|Tweet_length|\n",
      "+--------+----------+--------+-------+-------------+---------+------------+\n",
      "|       0|         0|    6152|      0|            0|        0|           0|\n",
      "+--------+----------+--------+-------+-------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan,when,count,col\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer,RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "#regextokenizer=RegexTokenizer(inputCol=\"OriginalTweet\", outputCol=\"words\",pattern=\"\\\\W\")\n",
    "tokenizer=Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"token_text\")\n",
    "stopremove=StopWordsRemover(inputCol=\"token_text\", outputCol=\"stop_tokens\")\n",
    "count_vec=CountVectorizer(inputCol=\"stop_tokens\", outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "#counttokens=udf(lambda words:len(words),IntegerType())\n",
    "\n",
    "#Convert labels to numeric\n",
    "labeltonum=StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up=VectorAssembler(inputCols=[\"tf_idf\",\"Tweet_length\"], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "nb=NaiveBayes()\n",
    "rf=RandomForestClassifier(numTrees=100)\n",
    "dtc=DecisionTreeClassifier(maxDepth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline= Pipeline(stages=[labeltonum, tokenizer, stopremove,count_vec, idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner=data_prep_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|         Sentiment|Tweet_length|label|          token_text|         stop_tokens|               c_vec|              tf_idf|            features|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|         111|  2.0|[@menyrbie, @phil...|[@menyrbie, @phil...|(78305,[12124,289...|(78305,[12124,289...|(78306,[12124,289...|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...|          Positive|         237|  0.0|[advice, talk, to...|[advice, talk, ne...|(78305,[13,14,133...|(78305,[13,14,133...|(78306,[13,14,133...|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...|          Positive|         131|  0.0|[coronavirus, aus...|[coronavirus, aus...|(78305,[8,14,37,7...|(78305,[8,14,37,7...|(78306,[8,14,37,7...|\n",
      "|    3804|     48756|ÃœT: 36.319708,-82...|16-03-2020|As news of the re...|          Positive|         249|  0.0|[as, news, of, th...|[news, regionÂ’s, ...|(78305,[7,8,31,47...|(78305,[7,8,31,47...|(78306,[7,8,31,47...|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...|          Positive|         184|  0.0|[\"cashier, at, gr...|[\"cashier, grocer...|(78305,[3,6,18,60...|(78305,[3,6,18,60...|(78306,[3,6,18,60...|\n",
      "|    3807|     48759|     Atlanta, GA USA|16-03-2020|Due to COVID-19 o...|          Positive|         280|  0.0|[due, to, covid-1...|[due, covid-19, r...|(78305,[1,6,8,13,...|(78305,[1,6,8,13,...|(78306,[1,6,8,13,...|\n",
      "|    3808|     48760|    BHAVNAGAR,GUJRAT|16-03-2020|For corona preven...|          Negative|         267|  1.0|[for, corona, pre...|[corona, preventi...|(78305,[11,13,14,...|(78305,[11,13,14,...|(78306,[11,13,14,...|\n",
      "|    3809|     48761|      Makati, Manila|16-03-2020|All month there h...|           Neutral|         276|  2.0|[all, month, ther...|[month, crowding,...|(78305,[48,70,149...|(78305,[48,70,149...|(78306,[48,70,149...|\n",
      "|    3811|     48763|          Horningsea|16-03-2020|#horningsea is a ...|Extremely Positive|         278|  3.0|[#horningsea, is,...|[#horningsea, car...|(78305,[13,14,23,...|(78305,[13,14,23,...|(78306,[13,14,23,...|\n",
      "|    3813|     48765|                null|16-03-2020|ADARA Releases CO...|          Positive|         185|  0.0|[adara, releases,...|[adara, releases,...|(78305,[8,10,23,5...|(78305,[8,10,23,5...|(78306,[8,10,23,5...|\n",
      "|    3818|     48770|          Denver, CO|16-03-2020|For those who are...|          Positive|         184|  0.0|[for, those, who,...|[struggling,, ple...|(78305,[4,8,24,38...|(78305,[4,8,24,38...|(78306,[4,8,24,38...|\n",
      "|    3819|     48771|southampton soxx xxx|16-03-2020|with 100  nations...|Extremely Negative|         251|  4.0|[with, 100, , nat...|[100, , nations, ...|(78305,[1,4,9,11,...|(78305,[1,4,9,11,...|(78306,[1,4,9,11,...|\n",
      "|    3823|     48775|    Downstage centre|16-03-2020|@10DowningStreet ...|          Negative|         255|  1.0|[@10downingstreet...|[@10downingstreet...|(78305,[4,21,44,7...|(78305,[4,21,44,7...|(78306,[4,21,44,7...|\n",
      "|    3824|     48776|              London|16-03-2020|UK #consumer poll...|Extremely Positive|         278|  3.0|[uk, #consumer, p...|[uk, #consumer, p...|(78305,[10,37,54,...|(78305,[10,37,54,...|(78306,[10,37,54,...|\n",
      "|    3825|     48777|      Ketchum, Idaho|16-03-2020|In preparation fo...|          Negative|         202|  1.0|[in, preparation,...|[preparation, hig...|(78305,[4,8,24,33...|(78305,[4,8,24,33...|(78306,[4,8,24,33...|\n",
      "|    3826|     48778| Everywhere You Are!|16-03-2020|This morning I te...|Extremely Negative|         279|  4.0|[this, morning, i...|[morning, tested,...|(78305,[1,7,11,36...|(78305,[1,7,11,36...|(78306,[1,7,11,36...|\n",
      "|    3829|     48781|                null|16-03-2020|There Is of in th...|          Negative|         114|  1.0|[there, is, of, i...|[country, , empty...|(78305,[1,4,7,34,...|(78305,[1,4,7,34,...|(78306,[1,4,7,34,...|\n",
      "|    3834|     48786|             Sverige|16-03-2020|Went to the super...|           Neutral|         122|  2.0|[went, to, the, s...|[went, supermarke...|(78305,[5,47,48,6...|(78305,[5,47,48,6...|(78306,[5,47,48,6...|\n",
      "|    3836|     48788|              Canada|16-03-2020|Worried about the...|          Positive|         225|  0.0|[worried, about, ...|[worried, impact,...|(78305,[8,12,23,2...|(78305,[8,12,23,2...|(78306,[8,12,23,2...|\n",
      "|    3837|     48789|                null|16-03-2020|my wife works ret...|          Negative|         288|  1.0|[my, wife, works,...|[wife, works, ret...|(78305,[6,28,33,9...|(78305,[6,28,33,9...|(78306,[6,28,33,9...|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=clean_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  2.0|(78306,[12124,289...|\n",
      "|  0.0|(78306,[13,14,133...|\n",
      "|  0.0|(78306,[8,14,37,7...|\n",
      "|  0.0|(78306,[7,8,31,47...|\n",
      "|  0.0|(78306,[3,6,18,60...|\n",
      "|  0.0|(78306,[1,6,8,13,...|\n",
      "|  1.0|(78306,[11,13,14,...|\n",
      "|  2.0|(78306,[48,70,149...|\n",
      "|  3.0|(78306,[13,14,23,...|\n",
      "|  0.0|(78306,[8,10,23,5...|\n",
      "|  0.0|(78306,[4,8,24,38...|\n",
      "|  4.0|(78306,[1,4,9,11,...|\n",
      "|  1.0|(78306,[4,21,44,7...|\n",
      "|  3.0|(78306,[10,37,54,...|\n",
      "|  1.0|(78306,[4,8,24,33...|\n",
      "|  4.0|(78306,[1,7,11,36...|\n",
      "|  1.0|(78306,[1,4,7,34,...|\n",
      "|  2.0|(78306,[5,47,48,6...|\n",
      "|  0.0|(78306,[8,12,23,2...|\n",
      "|  1.0|(78306,[6,28,33,9...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split\n",
    "(training, testing)=clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o148.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 57.0 failed 1 times, most recent failure: Lost task 2.0 in stage 57.0 (TID 689, master, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3682/1391255814.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3663/1325188422.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$2260/254281789.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2042/261322767.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:135)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:114)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3682/1391255814.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3663/1325188422.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$2260/254281789.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2042/261322767.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-34920a6a621b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentiment_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o148.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 57.0 failed 1 times, most recent failure: Lost task 2.0 in stage 57.0 (TID 689, master, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3682/1391255814.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3663/1325188422.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$2260/254281789.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2042/261322767.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:135)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:114)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3682/1391255814.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3663/1325188422.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$2260/254281789.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2042/261322767.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "sentiment_type=dtc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=spam_predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval=MulticlassClassificationEvaluator()\n",
    "acc=acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy of the model is::\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer=Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"tokenized_text\")\n",
    "#counttokens=udf(lambda tokenized_text:len(tokenized_text),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|       OriginalTweet|               words|NoOfWords|\n",
      "+--------------------+--------------------+---------+\n",
      "|@MeNyrbie @Phil_G...|[menyrbie, phil_g...|       17|\n",
      "|advice Talk to yo...|[advice, talk, to...|       38|\n",
      "|Coronavirus Austr...|[coronavirus, aus...|       18|\n",
      "|As news of the re...|[as, news, of, th...|       41|\n",
      "|\"Cashier at groce...|[cashier, at, gro...|       33|\n",
      "|Due to COVID-19 o...|[due, to, covid, ...|       50|\n",
      "|For corona preven...|[for, corona, pre...|       44|\n",
      "|All month there h...|[all, month, ther...|       43|\n",
      "|#horningsea is a ...|[horningsea, is, ...|       48|\n",
      "|ADARA Releases CO...|[adara, releases,...|       30|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenized.select(\"OriginalTweet\",\"words\").withColumn(\"NoOfWords\",counttokens(col(\"words\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|Tweet_length|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|         111|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...| Positive|         237|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|         131|\n",
      "|    3804|     48756|ÃœT: 36.319708,-82...|16-03-2020|As news of the re...| Positive|         249|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...| Positive|         184|\n",
      "+--------+----------+--------------------+----------+--------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|OriginalTweet                                                                                                                                                                                                                                                                                   |words                                                                                                                                                                                                                                                                                                                                      |NoOfWords|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8                                                                                                                                                                                 |[menyrbie, phil_gahan, chrisitv, https, t, co, ifz9fan2pa, and, https, t, co, xx6ghgfzcc, and, https, t, co, i2nlzdxno8]                                                                                                                                                                                                                   |17       |\n",
      "|advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order                                                   |[advice, talk, to, your, neighbours, family, to, exchange, phone, numbers, create, contact, list, with, phone, numbers, of, neighbours, schools, employer, chemist, gp, set, up, online, shopping, accounts, if, poss, adequate, supplies, of, regular, meds, but, not, over, order]                                                       |38       |\n",
      "|Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P                                                                                                                                                             |[coronavirus, australia, woolworths, to, give, elderly, disabled, dedicated, shopping, hours, amid, covid, 19, outbreak, https, t, co, binca9vp8p]                                                                                                                                                                                         |18       |\n",
      "|As news of the regionÂ’s first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU                                       |[as, news, of, the, region, s, first, confirmed, covid, 19, case, came, out, of, sullivan, county, last, week, people, flocked, to, area, stores, to, purchase, cleaning, supplies, hand, sanitizer, food, toilet, paper, and, other, goods, tim_dodson, reports, https, t, co, cfxch7a2lu]                                                |41       |\n",
      "|\"Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"\"I'm in Civics class so I know what I'm talking about\"\". https://t.co/ieFDNeHgDO\"                                                                                                        |[cashier, at, grocery, store, was, sharing, his, insights, on, covid_19, to, prove, his, credibility, he, commented, i, m, in, civics, class, so, i, know, what, i, m, talking, about, https, t, co, iefdnehgdo]                                                                                                                           |33       |\n",
      "|Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i        |[due, to, covid, 19, our, retail, store, and, classroom, in, atlanta, will, not, be, open, for, walk, in, business, or, classes, for, the, next, two, weeks, beginning, monday, march, 16, we, will, continue, to, process, online, and, phone, orders, as, normal, thank, you, for, your, understanding, https, t, co, kw91zj5o5i]        |50       |\n",
      "|For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona                     |[for, corona, prevention, we, should, stop, to, buy, things, with, the, cash, and, should, use, online, payment, methods, because, corona, can, spread, through, the, notes, also, we, should, prefer, online, shopping, from, our, home, it, s, time, to, fight, against, covid, 19, govindia, indiafightscorona]                         |44       |\n",
      "|All month there hasn't been crowding in the supermarkets or restaurants, however reducing all the hours and closing the malls means everyone is now using the same entrance and dependent on a single supermarket. #manila #lockdown #covid2019 #Philippines https://t.co/HxWs9LAnF9            |[all, month, there, hasn, t, been, crowding, in, the, supermarkets, or, restaurants, however, reducing, all, the, hours, and, closing, the, malls, means, everyone, is, now, using, the, same, entrance, and, dependent, on, a, single, supermarket, manila, lockdown, covid2019, philippines, https, t, co, hxws9lanf9]                   |43       |\n",
      "|#horningsea is a caring community. LetÂ’s ALL look after the less capable in our village and ensure they stay healthy. Bringing shopping to their doors, help with online shopping and self isolation if you have symptoms or been exposed to somebody who has. https://t.co/lsGrXXhjhh          |[horningsea, is, a, caring, community, let, s, all, look, after, the, less, capable, in, our, village, and, ensure, they, stay, healthy, bringing, shopping, to, their, doors, help, with, online, shopping, and, self, isolation, if, you, have, symptoms, or, been, exposed, to, somebody, who, has, https, t, co, lsgrxxhjhh]           |48       |\n",
      "|ADARA Releases COVID-19 Resource Center for Travel Brands: Insights Help Travel Brands Stay Up-To-Date on Consumer Travel Behavior Trends https://t.co/PnA797jDKV https://t.co/dQox6uSihz                                                                                                       |[adara, releases, covid, 19, resource, center, for, travel, brands, insights, help, travel, brands, stay, up, to, date, on, consumer, travel, behavior, trends, https, t, co, pna797jdkv, https, t, co, dqox6usihz]                                                                                                                        |30       |\n",
      "|For those who aren't struggling, please consider donating to a food bank or a nonprofit. The demand for these services will increase as COVID-19 impacts jobs, and people's way of life.                                                                                                        |[for, those, who, aren, t, struggling, please, consider, donating, to, a, food, bank, or, a, nonprofit, the, demand, for, these, services, will, increase, as, covid, 19, impacts, jobs, and, people, s, way, of, life]                                                                                                                    |34       |\n",
      "|with 100  nations inficted with  covid  19  the world must  not  play fair with china  100 goverments must demand  china  adopts new guilde  lines on food safty  the  chinese  goverment  is guilty of  being  irosponcible   with life  on a global scale                                     |[with, 100, nations, inficted, with, covid, 19, the, world, must, not, play, fair, with, china, 100, goverments, must, demand, china, adopts, new, guilde, lines, on, food, safty, the, chinese, goverment, is, guilty, of, being, irosponcible, with, life, on, a, global, scale]                                                         |41       |\n",
      "|@10DowningStreet @grantshapps what is being done to ensure food and other essential products are being re-stocked at supermarkets and panic buying actively discouraged? It cannot be left to checkout staff to police the actions of the selfish and profiteer                                 |[10downingstreet, grantshapps, what, is, being, done, to, ensure, food, and, other, essential, products, are, being, re, stocked, at, supermarkets, and, panic, buying, actively, discouraged, it, cannot, be, left, to, checkout, staff, to, police, the, actions, of, the, selfish, and, profiteer]                                      |40       |\n",
      "|UK #consumer poll indicates the majority expect #covid19's impact to last 4-12 months (at 12 March). We expect this to increase at the next #tracker... See full results of the @RetailX Coronavirus Consumer Confidence Tracker here: https://t.co/K3uJlcjqDB https://t.co/9G3kgqIXJ8          |[uk, consumer, poll, indicates, the, majority, expect, covid19, s, impact, to, last, 4, 12, months, at, 12, march, we, expect, this, to, increase, at, the, next, tracker, see, full, results, of, the, retailx, coronavirus, consumer, confidence, tracker, here, https, t, co, k3ujlcjqdb, https, t, co, 9g3kgqixj8]                     |46       |\n",
      "|In preparation for higher demand and a potential food shortage, The Hunger Coalition purchased 10 percent more food and implemented new protocols due to the COVID-19 coronavirus. https://t.co/5CecYtLnYn                                                                                      |[in, preparation, for, higher, demand, and, a, potential, food, shortage, the, hunger, coalition, purchased, 10, percent, more, food, and, implemented, new, protocols, due, to, the, covid, 19, coronavirus, https, t, co, 5cecytlnyn]                                                                                                    |32       |\n",
      "|This morning I tested positive for Covid 19. I feel ok, I have no symptoms so far but have been isolated since I found out about my possible exposure to the virus.  Stay home people and be pragmatic. I will keep you updated on how IÂ’m doing ???? No panic. https://t.co/Lg7HVMZglZ         |[this, morning, i, tested, positive, for, covid, 19, i, feel, ok, i, have, no, symptoms, so, far, but, have, been, isolated, since, i, found, out, about, my, possible, exposure, to, the, virus, stay, home, people, and, be, pragmatic, i, will, keep, you, updated, on, how, i, m, doing, no, panic, https, t, co, lg7hvmzglz]          |54       |\n",
      "|There Is of in the Country  The more empty shelves people see the more buying ensues the more food is out of stock                                                                                                                                                                              |[there, is, of, in, the, country, the, more, empty, shelves, people, see, the, more, buying, ensues, the, more, food, is, out, of, stock]                                                                                                                                                                                                  |23       |\n",
      "|Went to the supermarket yesterday and the toilet paper was gone. Has this anything to do with the Corona virus? #COVID2019                                                                                                                                                                      |[went, to, the, supermarket, yesterday, and, the, toilet, paper, was, gone, has, this, anything, to, do, with, the, corona, virus, covid2019]                                                                                                                                                                                              |21       |\n",
      "|Worried about the impact of the current COVID-19 pandemic on your finances? WeÂ’ve just published some tips to help you manage your money during these challenging times. #COVID19 https://t.co/3jKK3CqXfQ https://t.co/EbEnURmmJS                                                               |[worried, about, the, impact, of, the, current, covid, 19, pandemic, on, your, finances, we, ve, just, published, some, tips, to, help, you, manage, your, money, during, these, challenging, times, covid19, https, t, co, 3jkk3cqxfq, https, t, co, ebenurmmjs]                                                                          |38       |\n",
      "|my wife works retail&amp;a customer came in yesterday, coughing everywhere, saying they have CoVid-19. They requested a deep clean of the store - her company objected to due to cost, recommending the team spray disinfectant&amp;clean themselves. we're gonna die/get sick due to capitalism|[my, wife, works, retail, amp, a, customer, came, in, yesterday, coughing, everywhere, saying, they, have, covid, 19, they, requested, a, deep, clean, of, the, store, her, company, objected, to, due, to, cost, recommending, the, team, spray, disinfectant, amp, clean, themselves, we, re, gonna, die, get, sick, due, to, capitalism]|49       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenized.select(\"OriginalTweet\",\"words\").withColumn(\"NoOfWords\",counttokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
